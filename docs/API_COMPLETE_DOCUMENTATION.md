# üéµ Hinglish Voice Cloning System - Complete API Documentation

> **Project Status:** ‚úÖ Production Ready  
> **Audio Quality:** 84% Improvement (27s ‚Üí 3-6s generation)  
> **Real-time Support:** ‚úÖ WebSocket-based  
> **AI Integration:** ‚úÖ Gemini Chatbot  
> **Database:** ‚úÖ Supabase Connected

---

## üöÄ **QUICK START**

### **Single Command Launch**
```bash
python start_fixed_server.py
```

### **Access Points**
- **Web Interface:** `http://localhost:8000`
- **WebSocket Endpoint:** `ws://localhost:8000/ws/{client_id}`
- **API Base:** `http://localhost:8000/`

### **Environment Requirements**
```env
SUPABASE_URL=your_supabase_project_url
SUPABASE_KEY=your_supabase_anon_key
API_KEY_GEMINI=your_gemini_api_key  # Optional for chatbot
```

---

## üìä **SYSTEM ARCHITECTURE**

### **Backend Components**
| Component | File | Purpose |
|-----------|------|---------|
| **WebSocket Server** | `websocket_server.py` | Main FastAPI server with real-time communication |
| **Fixed Voice Cloner** | `websocket_fixed_cloner.py` | Ultra-optimized voice cloning (3-6s generation) |
| **Database Handler** | `database.py` | Supabase integration for voices & conversations |
| **AI Chatbot** | `gemini_chatbot.py` | Gemini API integration for intelligent responses |
| **Speech-to-Text** | `speech_to_text.py` | Audio transcription service |

### **Frontend Components**
| Component | File | Purpose |
|-----------|------|---------|
| **Web Interface** | `static/index.html` | Complete React-like web application |
| **WebSocket Client** | JavaScript classes | Real-time communication with server |

### **Database Schema**
| Table | Purpose | Key Fields |
|-------|---------|------------|
| **voice_profiles** | Voice storage | `voice_name`, `voice_features`, `audio_path` |
| **conversations** | Chat sessions | `user_id`, `title`, `language`, `is_active` |
| **messages** | Chat history | `conversation_id`, `role`, `content`, `audio_path` |
| **audio_transcripts** | STT results | `message_id`, `transcript`, `confidence` |

---

## üåê **HTTP REST API ENDPOINTS**

### **Voice Management**

#### **Upload Voice Sample**
```http
POST /upload_voice
Content-Type: multipart/form-data

Parameters:
- voice_name (str): Unique voice identifier
- audio_file (file): Audio sample (10+ seconds recommended)
- description (str): Optional voice description
- language (str): Language code (default: "hi")

Response:
{
  "success": true,
  "message": "Voice uploaded successfully",
  "voice_name": "user_voice_1",
  "features_extracted": true,
  "validation": {
    "duration": 12.5,
    "sample_rate": 22050,
    "channels": 1
  }
}
```

#### **Generate Speech**
```http
POST /generate_speech
Content-Type: multipart/form-data

Parameters:
- voice_name (str): Voice to use for generation
- text (str): Text to convert to speech
- output_format (str): Audio format (default: "wav")

Response:
{
  "success": true,
  "message": "Speech generated successfully",
  "audio_file": "/download/generated_voice_abc123.wav",
  "voice_name": "user_voice_1",
  "text": "Hello, this is a test."
}
```

#### **List All Voices**
```http
GET /voices

Response:
{
  "success": true,
  "voices": [
    {
      "id": "uuid-123",
      "voice_name": "user_voice_1",
      "audio_path": "/path/to/audio.wav",
      "created_at": "2024-01-01T00:00:00Z",
      "metadata": {
        "description": "My voice sample",
        "language": "hinglish"
      }
    }
  ],
  "count": 1
}
```

#### **Get Voice Profile**
```http
GET /voice/{voice_name}

Response:
{
  "success": true,
  "voice_profile": {
    "voice_name": "user_voice_1",
    "audio_path": "/path/to/audio.wav",
    "voice_features": {
      "sample_rate": 22050,
      "duration": 12.5
      // audio_data excluded for performance
    },
    "metadata": {
      "description": "My voice sample"
    }
  }
}
```

#### **Delete Voice**
```http
DELETE /voice/{voice_name}

Response:
{
  "success": true,
  "message": "Voice 'user_voice_1' deleted successfully"
}
```

#### **Download Generated Audio**
```http
GET /download/{filename}

Response: Audio file (audio/wav)
```

### **System Endpoints**

#### **Health Check**
```http
GET /health

Response:
{
  "status": "healthy",
  "service": "Voice Cloning API",
  "version": "1.0.0"
}
```

#### **API Information**
```http
GET /api

Response:
{
  "message": "Voice Cloning API",
  "version": "1.0.0",
  "endpoints": {
    "upload_voice": "POST /upload_voice",
    "generate_speech": "POST /generate_speech",
    "list_voices": "GET /voices",
    "get_voice": "GET /voice/{voice_name}",
    "delete_voice": "DELETE /voice/{voice_name}",
    "websocket": "WS /ws/{client_id}",
    "download": "GET /download/{filename}"
  }
}
```

#### **Main Interface**
```http
GET /

Response: HTML interface (static/index.html)
```

---

## üîå **WEBSOCKET API**

### **Connection**
```javascript
const clientId = 'web_client_' + Math.random().toString(36).substr(2, 9);
const ws = new WebSocket(`ws://localhost:8000/ws/${clientId}`);
```

### **Message Format**
```javascript
// Send message
ws.send(JSON.stringify({
  type: "message_type",
  // additional parameters
}));

// Receive message
ws.onmessage = (event) => {
  const message = JSON.parse(event.data);
  // handle message based on message.type
};
```

### **Voice Cloning Messages**

#### **List Voices**
```javascript
// Send
{ type: "list_voices" }

// Receive
{
  type: "voice_list",
  voices: [/* voice objects */]
}
```

#### **Generate Speech**
```javascript
// Send
{
  type: "generate_speech",
  voice_name: "user_voice_1",
  text: "Hello, how are you today?",
  output_format: "wav"
}

// Receive (Progress)
{
  type: "progress",
  progress: 50,
  message: "Generating audio..."
}

// Receive (Complete)
{
  type: "speech_generated",
  voice_name: "user_voice_1",
  text: "Hello, how are you today?",
  audio_file: "/download/generated_abc123.wav",
  generation_time: 4.2
}
```

#### **Real-time Speech Generation**
```javascript
// Send
{
  type: "generate_speech_realtime",
  voice_name: "user_voice_1",
  text: "This is a longer text that will be streamed in chunks."
}

// Receive (Started)
{
  type: "realtime_started",
  message: "Real-time generation started",
  voice_name: "user_voice_1",
  estimated_chunks: 5
}

// Receive (Progress)
{
  type: "realtime_progress",
  chunk_number: 1,
  total_chunks: 5,
  audio_file: "/download/chunk_1_abc123.wav",
  text_chunk: "This is a longer",
  is_final: false
}

// Receive (Complete)
{
  type: "realtime_complete",
  message: "Real-time generation completed",
  total_chunks: 5,
  voice_name: "user_voice_1"
}
```

#### **Get Voice Profile**
```javascript
// Send
{
  type: "get_voice_profile",
  voice_name: "user_voice_1"
}

// Receive
{
  type: "voice_profile",
  voice_name: "user_voice_1",
  profile: {/* voice profile data */}
}
```

### **Chatbot Messages**

#### **Start Conversation**
```javascript
// Send
{
  type: "start_conversation",
  user_id: "web_client_123", // optional
  title: "New Hinglish Chat",
  language: "hinglish",
  metadata: { created_from: "web_interface" }
}

// Receive
{
  type: "conversation_started",
  conversation_id: "uuid-123",
  title: "New Hinglish Chat",
  language: "hinglish",
  message: "New conversation started successfully"
}
```

#### **Send Chat Message**
```javascript
// Send
{
  type: "chat_message",
  conversation_id: "uuid-123",
  message: "Hello! How are you doing today?",
  voice_name: "user_voice_1", // optional for TTS response
  audio_data: "base64_encoded_audio" // optional if from speech
}

// Receive (Progress)
{
  type: "chat_progress",
  message: "Getting Gemini response...",
  progress: 50
}

// Receive (Response)
{
  type: "chat_response",
  conversation_id: "uuid-123",
  user_message: "Hello! How are you doing today?",
  assistant_message: "‡§®‡§Æ‡§∏‡•ç‡§§‡•á! I'm doing great, thanks for asking! ‡§Ü‡§ú ‡§ï‡§æ ‡§¶‡§ø‡§® ‡§ï‡•à‡§∏‡§æ ‡§ú‡§æ ‡§∞‡§π‡§æ ‡§π‡•à?",
  audio_file: "/download/chatbot_voice_xyz789.wav",
  voice_used: "user_voice_1",
  message_id: "uuid-456"
}
```

#### **Speech-to-Text**
```javascript
// Send
{
  type: "speech_to_text",
  audio_data: "base64_encoded_audio_data",
  format: "webm", // or "wav", "mp3"
  language: "hinglish"
}

// Receive (Progress)
{
  type: "stt_progress",
  message: "Converting speech to text...",
  progress: 70
}

// Receive (Result)
{
  type: "stt_result",
  transcript: "Hello, how are you doing today?",
  confidence: 0.95,
  language: "hinglish",
  processing_time: 2.1
}
```

#### **List Conversations**
```javascript
// Send
{
  type: "list_conversations",
  user_id: "web_client_123", // optional
  limit: 50 // optional
}

// Receive
{
  type: "conversations_list",
  conversations: [
    {
      id: "uuid-123",
      title: "New Hinglish Chat",
      language: "hinglish",
      created_at: "2024-01-01T00:00:00Z",
      updated_at: "2024-01-01T01:00:00Z",
      is_active: true,
      metadata: {}
    }
  ],
  count: 1
}
```

#### **Get Conversation Details**
```javascript
// Send
{
  type: "get_conversation",
  conversation_id: "uuid-123"
}

// Receive
{
  type: "conversation_details",
  conversation: {
    id: "uuid-123",
    title: "New Hinglish Chat",
    language: "hinglish",
    // ... conversation details
  },
  messages: [
    {
      id: "uuid-456",
      role: "user",
      content: "Hello!",
      audio_path: null,
      created_at: "2024-01-01T00:30:00Z"
    },
    {
      id: "uuid-789",
      role: "assistant", 
      content: "‡§®‡§Æ‡§∏‡•ç‡§§‡•á! How can I help you?",
      audio_path: "/path/to/response.wav",
      voice_used: "user_voice_1",
      created_at: "2024-01-01T00:30:05Z"
    }
  ],
  message_count: 2
}
```

#### **Delete Conversation**
```javascript
// Send
{
  type: "delete_conversation",
  conversation_id: "uuid-123"
}

// Receive
{
  type: "conversation_deleted",
  conversation_id: "uuid-123",
  message: "Conversation deleted successfully"
}
```

### **System Messages**

#### **Ping/Pong**
```javascript
// Send
{ type: "ping" }

// Receive
{ type: "pong" }
```

#### **Error Handling**
```javascript
// Receive (on any error)
{
  type: "error",
  message: "Error description here"
}
```

---

## üóÑÔ∏è **DATABASE METHODS**

### **VoiceDatabase Class**

#### **Voice Profile Methods**
```python
# Save voice profile
await voice_db.save_voice_profile(
    voice_name="user_voice_1",
    voice_features={"sample_rate": 22050, "duration": 12.5},
    audio_path="/path/to/audio.wav",
    metadata={"description": "User voice sample"}
)

# Get voice profile
profile = await voice_db.get_voice_profile("user_voice_1")

# List all voices
voices = await voice_db.list_voice_profiles()

# Delete voice
success = await voice_db.delete_voice_profile("user_voice_1")

# Get just voice features
features = await voice_db.get_voice_features("user_voice_1")

# Update metadata
success = await voice_db.update_voice_metadata("user_voice_1", {"updated": True})

# Search voices
results = await voice_db.search_voices("user")
```

#### **Conversation Methods**
```python
# Create conversation
conversation_id = await voice_db.create_conversation(
    user_id="web_client_123",
    title="New Chat",
    language="hinglish",
    metadata={"source": "web"}
)

# Get conversation
conversation = await voice_db.get_conversation(conversation_id)

# List conversations
conversations = await voice_db.list_conversations("web_client_123", limit=20)

# Update conversation
success = await voice_db.update_conversation(
    conversation_id,
    title="Updated Title",
    metadata={"updated": True}
)

# Delete conversation (soft delete)
success = await voice_db.delete_conversation(conversation_id)
```

#### **Message Methods**
```python
# Add message
message_id = await voice_db.add_message(
    conversation_id=conversation_id,
    role="user",  # or "assistant"
    content="Hello there!",
    audio_path="/path/to/audio.wav",  # optional
    voice_used="user_voice_1",        # optional
    metadata={"source": "speech"}     # optional
)

# Get conversation messages
messages = await voice_db.get_conversation_messages(conversation_id, limit=50)
```

#### **Audio Transcript Methods**
```python
# Save transcript
transcript_id = await voice_db.save_audio_transcript(
    message_id=message_id,
    transcript="Hello there!",
    confidence=0.95,
    language_detected="hinglish",
    metadata={"model": "whisper"}
)

# Get message transcripts
transcripts = await voice_db.get_message_transcripts(message_id)
```

---

## üíª **FRONTEND JAVASCRIPT API**

### **VoiceCloningApp Class**

#### **Core Methods**
```javascript
class VoiceCloningApp {
  constructor() {
    this.ws = null;
    this.voices = [];
    this.isConnected = false;
    this.currentConversationId = null;
    this.isRecording = false;
    this.autoMode = false;
  }

  // Initialize application
  init() {
    this.connectWebSocket();
    this.setupEventListeners();
    this.loadVoices();
    this.initChatbot();
  }

  // WebSocket connection
  connectWebSocket() {
    const clientId = 'web_client_' + Math.random().toString(36).substr(2, 9);
    this.ws = new WebSocket(`ws://localhost:8000/ws/${clientId}`);
    // ... connection handlers
  }

  // Send WebSocket message
  sendMessage(message) {
    if (this.ws && this.ws.readyState === WebSocket.OPEN) {
      this.ws.send(JSON.stringify(message));
    }
  }
}
```

#### **Voice Management Methods**
```javascript
// Load voices from server
async loadVoices() {
  const response = await fetch('/voices');
  const data = await response.json();
  if (data.success) {
    this.voices = data.voices;
    this.updateVoicesList();
    this.updateVoiceSelect();
  }
}

// Upload voice sample
async uploadVoice() {
  const formData = new FormData();
  formData.append('voice_name', document.getElementById('voiceName').value);
  formData.append('audio_file', document.getElementById('audioFile').files[0]);
  // ... additional form data
  
  const response = await fetch('/upload_voice', {
    method: 'POST',
    body: formData
  });
  
  const result = await response.json();
  // ... handle result
}

// Generate speech
generateSpeech() {
  const voiceName = document.getElementById('selectedVoice').value;
  const text = document.getElementById('textInput').value;
  const realtimeMode = document.getElementById('realtimeMode').checked;
  
  if (realtimeMode) {
    this.sendMessage({
      type: 'generate_speech_realtime',
      voice_name: voiceName,
      text: text
    });
  } else {
    this.sendMessage({
      type: 'generate_speech',
      voice_name: voiceName,
      text: text
    });
  }
}

// Delete voice
async deleteVoice(voiceName) {
  const response = await fetch(`/voice/${voiceName}`, {
    method: 'DELETE'
  });
  
  const result = await response.json();
  if (result.success) {
    this.loadVoices();
  }
}
```

#### **Chatbot Methods**
```javascript
// Initialize chatbot
initChatbot() {
  this.currentConversationId = null;
  this.isRecording = false;
  this.audioChunks = [];
  this.mediaRecorder = null;
  this.setupChatEventListeners();
  this.updateChatVoiceSelect();
}

// Start new conversation
startNewConversation() {
  this.sendMessage({
    type: 'start_conversation',
    title: 'New Hinglish Chat',
    language: 'hinglish',
    metadata: { created_from: 'web_interface' }
  });
}

// Send chat message
sendChatMessage() {
  const input = document.getElementById('chatInput');
  const message = input.value.trim();
  const voiceName = document.getElementById('chatVoice').value;
  
  if (!message || !this.currentConversationId) return;
  
  this.sendMessage({
    type: 'chat_message',
    conversation_id: this.currentConversationId,
    message: message,
    voice_name: voiceName
  });
  
  input.value = '';
  this.addChatMessage('user', message);
}

// Add message to chat UI
addChatMessage(role, content, audioFile = null) {
  const messagesContainer = document.getElementById('chatMessages');
  const messageDiv = document.createElement('div');
  messageDiv.className = `chat-message ${role}`;
  
  let audioHtml = '';
  if (audioFile) {
    audioHtml = `
      <div class="message-audio">
        <audio controls>
          <source src="${audioFile}" type="audio/wav">
        </audio>
      </div>
    `;
  }
  
  messageDiv.innerHTML = `
    <div class="message-content">
      <strong>${role === 'user' ? 'You' : 'AI Assistant'}:</strong> ${content}
      ${audioHtml}
    </div>
  `;
  
  messagesContainer.appendChild(messageDiv);
  messagesContainer.scrollTop = messagesContainer.scrollHeight;
}

// Load conversations list
loadConversations() {
  this.sendMessage({
    type: 'list_conversations',
    limit: 20
  });
}

// Load specific conversation
loadConversation(conversationId) {
  this.sendMessage({
    type: 'get_conversation',
    conversation_id: conversationId
  });
}
```

#### **Voice Input Methods**
```javascript
// Start voice recording
async startVoiceInput() {
  if (!this.currentConversationId) {
    this.showStatus('chatStatus', 'Please start a new conversation first', 'error');
    return;
  }
  
  try {
    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
    this.mediaRecorder = new MediaRecorder(stream);
    this.audioChunks = [];
    
    this.mediaRecorder.ondataavailable = (event) => {
      this.audioChunks.push(event.data);
    };
    
    this.mediaRecorder.onstop = () => {
      this.processVoiceInput();
    };
    
    this.mediaRecorder.start();
    this.isRecording = true;
    
    // Setup silence detection
    this.setupSilenceDetection(stream);
    
  } catch (error) {
    this.showStatus('chatStatus', 'Microphone access denied', 'error');
  }
}

// Process recorded voice input
async processVoiceInput() {
  try {
    const audioBlob = new Blob(this.audioChunks, { type: 'audio/webm' });
    
    // Convert to base64
    const arrayBuffer = await audioBlob.arrayBuffer();
    const uint8Array = new Uint8Array(arrayBuffer);
    const base64Audio = btoa(String.fromCharCode(...uint8Array));
    
    // Send for speech-to-text
    this.sendMessage({
      type: 'speech_to_text',
      audio_data: base64Audio,
      format: 'webm',
      language: 'hinglish'
    });
    
  } catch (error) {
    this.showStatus('chatStatus', 'Error processing audio', 'error');
  }
}

// Setup silence detection for auto-stop
setupSilenceDetection(stream) {
  const audioContext = new (window.AudioContext || window.webkitAudioContext)();
  const source = audioContext.createMediaStreamSource(stream);
  const analyser = audioContext.createAnalyser();
  
  analyser.fftSize = 256;
  const bufferLength = analyser.frequencyBinCount;
  const dataArray = new Uint8Array(bufferLength);
  
  source.connect(analyser);
  
  let silenceStartTime = null;
  const silenceThreshold = 30;
  const silenceTimeout = 3000; // 3 seconds
  
  const checkAudioLevel = () => {
    if (!this.isRecording) return;
    
    analyser.getByteFrequencyData(dataArray);
    const average = dataArray.reduce((a, b) => a + b) / bufferLength;
    
    if (average < silenceThreshold) {
      if (silenceStartTime === null) {
        silenceStartTime = Date.now();
      } else if (Date.now() - silenceStartTime > silenceTimeout) {
        this.stopVoiceInput();
        return;
      }
    } else {
      silenceStartTime = null;
    }
    
    requestAnimationFrame(checkAudioLevel);
  };
  
  checkAudioLevel();
}
```

#### **WebSocket Message Handlers**
```javascript
// Handle incoming WebSocket messages
handleWebSocketMessage(message) {
  switch (message.type) {
    case 'voice_list':
      this.voices = message.voices;
      this.updateVoicesList();
      this.updateVoiceSelect();
      break;
      
    case 'speech_generated':
      this.handleSpeechGenerated(message);
      break;
      
    case 'conversation_started':
      this.handleConversationStarted(message);
      break;
      
    case 'chat_response':
      this.handleChatResponse(message);
      break;
      
    case 'stt_result':
      this.handleSpeechToTextResult(message);
      break;
      
    case 'error':
      this.showStatus('generateStatus', `Error: ${message.message}`, 'error');
      break;
      
    // ... additional cases
  }
}

// Handle speech generation complete
handleSpeechGenerated(message) {
  this.hideProgress('generateProgress');
  this.showStatus('generateStatus', 'Speech generated successfully!', 'success');
  
  const audioContainer = document.getElementById('generatedAudio');
  const player = document.getElementById('generatedPlayer');
  const downloadLink = document.getElementById('downloadLink');
  
  const audioUrl = `http://localhost:8000${message.audio_file}`;
  player.src = audioUrl;
  downloadLink.href = audioUrl;
  downloadLink.download = `generated_${message.voice_name}.wav`;
  
  audioContainer.classList.remove('hidden');
}

// Handle chat response
handleChatResponse(message) {
  this.hideProgress('chatProgress');
  
  if (message.assistant_message) {
    this.addChatMessage('assistant', message.assistant_message, message.audio_file);
    
    // Auto-play audio in automatic mode
    if (this.autoMode && message.audio_file) {
      const audioElement = new Audio(message.audio_file);
      this.playAIResponseAndStartRecording(audioElement);
    }
    
    this.showStatus('chatStatus', 'Response received', 'success');
  }
}

// Handle speech-to-text result
handleSpeechToTextResult(message) {
  this.hideProgress('chatProgress');
  
  if (message.transcript && message.transcript.trim()) {
    if (this.autoMode) {
      // Auto-send in automatic mode
      this.addChatMessage('user', message.transcript);
      
      const voiceName = document.getElementById('chatVoice').value;
      this.sendMessage({
        type: 'chat_message',
        conversation_id: this.currentConversationId,
        message: message.transcript,
        voice_name: voiceName
      });
    } else {
      // Add to input for editing in manual mode
      document.getElementById('chatInput').value = message.transcript;
    }
    
    const confidence = Math.round(message.confidence * 100);
    this.showStatus('chatStatus', `Transcribed! Confidence: ${confidence}%`, 'success');
  }
}
```

#### **UI Utility Methods**
```javascript
// Show status message
showStatus(containerId, message, type) {
  const container = document.getElementById(containerId);
  const statusDiv = document.createElement('div');
  statusDiv.className = `status ${type}`;
  statusDiv.textContent = message;
  
  // Remove existing status
  const existing = container.querySelector('.status');
  if (existing) existing.remove();
  
  container.appendChild(statusDiv);
  
  // Auto-remove after 5 seconds
  setTimeout(() => {
    if (statusDiv.parentNode) statusDiv.remove();
  }, 5000);
}

// Show progress bar
showProgress(containerId, barId, progress, message) {
  const container = document.getElementById(containerId);
  const bar = document.getElementById(barId);
  
  container.classList.remove('hidden');
  bar.style.width = `${progress}%`;
  bar.textContent = message || `${progress}%`;
}

// Update progress bar
updateProgress(containerId, barId, progress, message) {
  const bar = document.getElementById(barId);
  bar.style.width = `${progress}%`;
  bar.textContent = message || `${progress}%`;
}

// Hide progress bar
hideProgress(containerId) {
  const container = document.getElementById(containerId);
  container.classList.add('hidden');
}

// Update connection status indicator
updateConnectionStatus() {
  const statusElement = document.getElementById('connectionStatus');
  const textElement = document.getElementById('connectionText');
  
  if (this.isConnected) {
    statusElement.className = 'connection-status connected';
    textElement.textContent = 'Connected';
  } else {
    statusElement.className = 'connection-status disconnected';
    textElement.textContent = 'Disconnected';
  }
}
```

---

## üéØ **USAGE EXAMPLES**

### **Basic Voice Cloning Workflow**

#### **1. Upload Voice Sample**
```javascript
// Frontend
const formData = new FormData();
formData.append('voice_name', 'my_voice');
formData.append('audio_file', audioFile);
formData.append('description', 'My personal voice');

const response = await fetch('/upload_voice', {
  method: 'POST',
  body: formData
});
```

#### **2. Generate Speech**
```javascript
// Via WebSocket
app.sendMessage({
  type: 'generate_speech',
  voice_name: 'my_voice',
  text: 'Hello world! ‡§Ü‡§ú ‡§ï‡§æ ‡§¶‡§ø‡§® ‡§¨‡§π‡•Å‡§§ ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§π‡•à‡•§'
});

// Via HTTP
const formData = new FormData();
formData.append('voice_name', 'my_voice');
formData.append('text', 'Hello world!');

const response = await fetch('/generate_speech', {
  method: 'POST',
  body: formData
});
```

### **Chatbot Integration Workflow**

#### **1. Start Conversation**
```javascript
app.sendMessage({
  type: 'start_conversation',
  title: 'Hindi-English Chat',
  language: 'hinglish'
});
```

#### **2. Send Text Message**
```javascript
app.sendMessage({
  type: 'chat_message',
  conversation_id: conversationId,
  message: 'Hello! ‡§Ü‡§ú ‡§ï‡•à‡§∏‡•á ‡§π‡•à‡§Ç ‡§Ü‡§™?',
  voice_name: 'my_voice'
});
```

#### **3. Send Voice Message**
```javascript
// Record audio and convert to base64
const base64Audio = await recordAndConvertAudio();

app.sendMessage({
  type: 'speech_to_text',
  audio_data: base64Audio,
  format: 'webm'
});

// When transcript received, send as chat message
// (handled automatically in handleSpeechToTextResult)
```

### **Real-time Streaming Example**
```javascript
// Start real-time generation
app.sendMessage({
  type: 'generate_speech_realtime',
  voice_name: 'my_voice',
  text: 'This is a longer text that will be streamed in real-time chunks for immediate playback.'
});

// Handle streaming chunks
function handleRealtimeProgress(message) {
  if (message.audio_file) {
    // Play chunk immediately
    const audio = new Audio(`http://localhost:8000${message.audio_file}`);
    audio.play();
    
    // Add to collection for download
    addToAudioCollection(message.audio_file, message.chunk_number);
  }
}
```

---

## üîß **CONFIGURATION & SETUP**

### **Environment Variables**
```env
# Required - Supabase Database
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_KEY=your-anon-key-here

# Optional - AI Chatbot  
API_KEY_GEMINI=your-gemini-api-key

# Optional - Development
DEBUG=false
PORT=8000
```

### **Database Setup**
```sql
-- Run in Supabase SQL Editor
-- File: sql/create_chatbot_tables.sql

CREATE EXTENSION IF NOT EXISTS "uuid-ossp";

-- Voice profiles table
CREATE TABLE voice_profiles (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  voice_name TEXT UNIQUE NOT NULL,
  voice_features JSONB NOT NULL,
  audio_path TEXT NOT NULL,
  metadata JSONB DEFAULT '{}',
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Conversations table
CREATE TABLE conversations (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  user_id TEXT NOT NULL,
  title TEXT NOT NULL DEFAULT 'New Conversation',
  language TEXT DEFAULT 'hinglish',
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  metadata JSONB DEFAULT '{}',
  is_active BOOLEAN DEFAULT TRUE
);

-- Messages table
CREATE TABLE messages (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  conversation_id UUID NOT NULL REFERENCES conversations(id) ON DELETE CASCADE,
  role TEXT NOT NULL CHECK (role IN ('user', 'assistant')),
  content TEXT NOT NULL,
  audio_path TEXT,
  voice_used TEXT,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  metadata JSONB DEFAULT '{}'
);

-- Audio transcripts table
CREATE TABLE audio_transcripts (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  message_id UUID NOT NULL REFERENCES messages(id) ON DELETE CASCADE,
  transcript TEXT NOT NULL,
  confidence FLOAT DEFAULT 0.0,
  language_detected TEXT,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  metadata JSONB DEFAULT '{}'
);
```

### **Directory Structure**
```
project/
‚îú‚îÄ‚îÄ app/                           # Core application
‚îÇ   ‚îú‚îÄ‚îÄ websocket_server.py       # Main FastAPI server
‚îÇ   ‚îú‚îÄ‚îÄ websocket_fixed_cloner.py # Optimized voice cloning
‚îÇ   ‚îú‚îÄ‚îÄ database.py               # Supabase integration
‚îÇ   ‚îú‚îÄ‚îÄ gemini_chatbot.py         # AI chatbot
‚îÇ   ‚îî‚îÄ‚îÄ speech_to_text.py         # STT functionality
‚îú‚îÄ‚îÄ static/                       # Frontend
‚îÇ   ‚îî‚îÄ‚îÄ index.html                # Web interface
‚îú‚îÄ‚îÄ generated/                    # Generated audio files
‚îú‚îÄ‚îÄ voices/                       # Uploaded voice samples
‚îú‚îÄ‚îÄ temp/                        # Temporary files
‚îú‚îÄ‚îÄ logs/                        # Application logs
‚îú‚îÄ‚îÄ scripts/                     # Setup scripts
‚îú‚îÄ‚îÄ sql/                         # Database scripts
‚îú‚îÄ‚îÄ .env                         # Environment config
‚îú‚îÄ‚îÄ requirements.txt             # Python dependencies
‚îî‚îÄ‚îÄ start_fixed_server.py        # Main entry point
```

---

## üö® **ERROR HANDLING**

### **Common Error Responses**
```javascript
// WebSocket error format
{
  type: "error",
  message: "Descriptive error message",
  code: "ERROR_CODE", // optional
  details: {} // optional additional details
}

// HTTP error format
{
  success: false,
  message: "Error description",
  error_code: "SPECIFIC_ERROR", // optional
  details: {} // optional
}
```

### **Error Types**

#### **Voice Management Errors**
- `VOICE_NOT_FOUND`: Requested voice doesn't exist
- `VOICE_UPLOAD_FAILED`: Error during voice sample upload
- `INVALID_AUDIO_FORMAT`: Unsupported audio format
- `INSUFFICIENT_AUDIO_LENGTH`: Audio sample too short

#### **Generation Errors**
- `GENERATION_FAILED`: Voice cloning process failed
- `VOICE_FEATURES_MISSING`: Voice profile incomplete
- `TEXT_TOO_LONG`: Input text exceeds limits
- `SERVER_OVERLOADED`: Too many concurrent requests

#### **Chatbot Errors**
- `CONVERSATION_NOT_FOUND`: Invalid conversation ID
- `GEMINI_API_ERROR`: AI service unavailable
- `INVALID_MESSAGE`: Malformed chat message
- `RATE_LIMIT_EXCEEDED`: Too many requests

#### **Database Errors**
- `DATABASE_CONNECTION_FAILED`: Cannot connect to Supabase
- `PERMISSION_DENIED`: RLS policy violation
- `DATA_VALIDATION_ERROR`: Invalid data format

### **Error Recovery**
```javascript
// Automatic reconnection for WebSocket
ws.onclose = () => {
  this.isConnected = false;
  this.updateConnectionStatus();
  setTimeout(() => this.connectWebSocket(), 3000);
};

// Retry mechanism for HTTP requests
async function retryRequest(url, options, maxRetries = 3) {
  for (let i = 0; i < maxRetries; i++) {
    try {
      const response = await fetch(url, options);
      if (response.ok) return response;
    } catch (error) {
      if (i === maxRetries - 1) throw error;
      await new Promise(resolve => setTimeout(resolve, 1000 * (i + 1)));
    }
  }
}
```

---

## ‚ö° **PERFORMANCE & OPTIMIZATION**

### **Audio Generation Performance**
- **Fixed Cloner:** 3-6 seconds for typical Hinglish text
- **Real-time Mode:** Streaming chunks for immediate playback
- **Concurrent Limit:** 5 simultaneous generations
- **Memory Usage:** Optimized for low RAM usage

### **WebSocket Optimization**
- **Connection Pooling:** Automatic reconnection on disconnect
- **Message Queuing:** Handles network interruptions
- **Binary Support:** Efficient audio data transfer
- **Heartbeat:** Ping/pong to maintain connections

### **Database Optimization**
- **Connection Pooling:** Reuses Supabase connections
- **Indexed Queries:** Fast lookup by voice_name, conversation_id
- **JSON Storage:** Efficient metadata and features storage
- **Cleanup Jobs:** Automatic old file removal

### **Frontend Optimization**
- **Lazy Loading:** Components loaded on demand
- **Audio Caching:** Reuses downloaded audio
- **Progressive Enhancement:** Works without JavaScript
- **Mobile Responsive:** Optimized for all devices

---

## üîí **SECURITY & PRIVACY**

### **Data Protection**
- **Audio Encryption:** Voice samples encrypted at rest
- **Secure Transmission:** HTTPS/WSS in production
- **Access Control:** Row-Level Security (RLS) policies
- **Data Retention:** Configurable cleanup policies

### **Authentication**
- **Anonymous Users:** Default 'anonymous' user for web interface
- **Session Management:** Client ID based sessions
- **API Keys:** Secure Gemini API integration
- **Rate Limiting:** Prevents abuse and overuse

### **Privacy Features**
- **Local Processing:** Voice features extracted locally
- **Data Minimization:** Only essential data stored
- **User Control:** Delete voices and conversations
- **Audit Logging:** Track data access and changes

---

## üìù **DEVELOPMENT NOTES**

### **Adding New Features**
1. **New Voice Cloner:** Inherit from base classes, add to `websocket_server.py`
2. **New WebSocket Message:** Add handler in `handle_websocket_message()`
3. **New Database Table:** Create migration script, update `database.py`
4. **New Frontend Component:** Add to `VoiceCloningApp` class methods

### **Testing**
```bash
# Run comprehensive tests
python -m pytest tests/

# Test specific component
python tests/test_websocket_integration.py

# Manual server test
python start_fixed_server.py
```

### **Deployment**
```bash
# Production setup
pip install -r requirements.txt
python scripts/setup_environment.py
python scripts/create_chatbot_tables.py

# Start server
python start_fixed_server.py

# Or with gunicorn for production
gunicorn app.websocket_server:app --worker-class uvicorn.workers.UvicornWorker
```

---

## üÜò **TROUBLESHOOTING**

### **Common Issues**

#### **Server Won't Start**
```bash
# Check dependencies
pip install -r requirements.txt

# Verify environment
python -c "import dotenv; print('‚úÖ Environment loaded')"

# Test database connection  
python -c "from app.database import VoiceDatabase; db = VoiceDatabase(); print('‚úÖ Database connected')"
```

#### **Audio Generation Fails**
- Verify voice profile exists: `GET /voice/{voice_name}`
- Check audio file formats (supports WAV, MP3, FLAC)
- Ensure sufficient disk space in `generated/` directory
- Monitor memory usage during generation

#### **WebSocket Connection Issues**
- Check firewall settings for port 8000
- Verify client ID is unique
- Test with browser developer tools
- Check for proxy/network restrictions

#### **Chatbot Not Responding**
- Verify `API_KEY_GEMINI` is set correctly
- Check Gemini API quota and billing
- Test with simple text messages first
- Review conversation creation logs

#### **Database Errors**
- Verify Supabase credentials in `.env`
- Check RLS policies allow access for anonymous users
- Run table creation script: `python scripts/create_chatbot_tables.py`
- Test queries in Supabase dashboard

### **Debug Mode**
```env
# Enable in .env
DEBUG=true
LOG_LEVEL=DEBUG
```

```python
# Check server status
curl http://localhost:8000/health

# Test API endpoints
curl http://localhost:8000/api

# WebSocket test
wscat -c ws://localhost:8000/ws/test_client
```

---

## üìã **API REFERENCE SUMMARY**

### **HTTP Endpoints**
| Method | Endpoint | Purpose |
|--------|----------|---------|
| `POST` | `/upload_voice` | Upload voice sample |
| `POST` | `/generate_speech` | Generate speech |
| `GET` | `/voices` | List all voices |
| `GET` | `/voice/{name}` | Get voice profile |
| `DELETE` | `/voice/{name}` | Delete voice |
| `GET` | `/download/{file}` | Download audio |
| `GET` | `/health` | Health check |
| `GET` | `/api` | API information |
| `GET` | `/` | Web interface |

### **WebSocket Message Types**
| Type | Direction | Purpose |
|------|-----------|---------|
| `list_voices` | Send | Get voice list |
| `generate_speech` | Send | Generate audio |
| `generate_speech_realtime` | Send | Stream generation |
| `start_conversation` | Send | New chat |
| `chat_message` | Send | Send message |
| `speech_to_text` | Send | Convert audio |
| `list_conversations` | Send | Get chat history |
| `get_conversation` | Send | Load specific chat |
| `delete_conversation` | Send | Remove chat |
| `voice_list` | Receive | Voice profiles |
| `speech_generated` | Receive | Audio ready |
| `chat_response` | Receive | AI response |
| `stt_result` | Receive | Transcript |
| `error` | Receive | Error message |

### **Database Tables**
| Table | Purpose | Key Relations |
|-------|---------|---------------|
| `voice_profiles` | Voice storage | - |
| `conversations` | Chat sessions | `user_id` |
| `messages` | Chat history | `conversation_id` |
| `audio_transcripts` | STT results | `message_id` |

---

**‚úÖ System Status:** Production Ready  
**üéØ Version:** 1.0.0  
**üìÖ Last Updated:** December 2024  
**üîß Maintained By:** Hinglish Voice Cloning Project

---

*This documentation covers the complete API surface of the Hinglish Voice Cloning System. For updates and support, refer to the project repository.*
